{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intro to deep learning course\n",
    "# Basic Perceptron implementation in numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # used to plot\n",
    "\n",
    "\n",
    "# method for the forward propagation method\n",
    "class Layer:\n",
    "    def __init__ (self,activation,h_size, output_size,lr=0.05):\n",
    "        \n",
    "        self.activation = activation # activation function(string)\n",
    "        self.h_size = h_size # size of the input vector \n",
    "        self.lr = lr # learning rate \n",
    "        self.h_l = None # input vector\n",
    "        self.d_transfer = None # derivative of the transfer function\n",
    "        self.output_size = output_size\n",
    "        self.weights = np.random.rand(output_size, h_size)\n",
    "        self.weights = 5.0*np.matrix(self.weights) # weight matrix\n",
    "        self.bias_vec = np.matrix(np.random.rand(output_size))\n",
    "        self.gradient = np.matrix(np.zeros(output_size))\n",
    "        \n",
    "    #sigmoid function   \n",
    "    def sigmoid(self,x):\n",
    "        return 1.0/(1+np.exp(-x))\n",
    "    #derivative of the sigmoid function \n",
    "    def dsigmoid(self,x):\n",
    "        return np.multiply(x,(1.0-x))\n",
    "    \n",
    "    \n",
    "    #forward prop for the layer(accepts input from the previous layer)\n",
    "    #vectors are assumed to be row vector, vector transpose \n",
    "    def layer_forward(self,h_l): #input vector forward propagation\n",
    "        self.h_l = h_l\n",
    "        output = self.weights*self.h_l+self.bias_vec # assume the x vector is treated as a matrix instead of a numpy array\n",
    "        if (self.activation == \"sigmoid\"):\n",
    "            output = self.sigmoid(output)\n",
    "            self.d_transfer = self.dsigmoid(output) #  capture the derivative of the transfer function with respect to its input\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    #back prop for the layer (accept gradient from the next layer)\n",
    "    def layer_backprop(self,grad_l):\n",
    "        \n",
    "        back_grad_W = np.multiply(grad_l,self.d_transfer) # back grad is the derivative of the Loss function with respect to input h_l\n",
    "        # updating parameters with regularization \n",
    "        self.gradients = self.gradient -self.lr*(back_grad_W*self.h_l.T) #+lambda_val*np.linalg.norm(self.weights,2)\n",
    "        self.bias_vec = self.bias_vec -self.lr*grad_l\n",
    "        grad_l = grad_l\n",
    "        return back_grad_W.T\n",
    "    \n",
    "    #MSE  x and t are treated as matrices\n",
    "    \n",
    "def MSE(x,t):\n",
    "        y = (x-t).T*(x-t)\n",
    "        y = y/2.0*len(np.array(x).tolist())\n",
    "        return y\n",
    "        \n",
    "    \n",
    "def dMSEdx(y,t):\n",
    "        return (y-t)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.34626447 4.89000587 0.24276698]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9999982395007044]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Layer(activation = \"sigmoid\" , h_size =3,output_size=1)\n",
    "print(perceptron.weights)\n",
    "\n",
    "np.array(perceptron.layer_forward(np.matrix([1,2,3]).T)).tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]]\n",
      "[[1 2 3]]\n",
      "\n",
      "\n",
      "[[14]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.75]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.matrix([1,2,3]).T)\n",
    "print(np.matrix([1,2,3]))\n",
    "print(\"\\n\")\n",
    "print((np.matrix([1,2,3])*np.matrix([1,2,3]).T))\n",
    "MSE(np.matrix([1,2,3]).T,np.matrix([1.5,2,3.5]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[2.34626447, 4.89000587, 0.24276698]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = []\n",
    "x_input = []\n",
    "def equation(x,y,z):\n",
    "    return (2.0*x)+(3.0*y)+(1.0*z)+3.0\n",
    "for i in range(100):\n",
    "    x_input.append([i,i,i])\n",
    "    target.append([equation(i,i,i)])\n",
    "print(np.shape(x_input))\n",
    "#print(np.shape(np.array(np.matrix(target)).tolist()))\n",
    "#print(type(np.array(np.matrix(target)).tolist()))\n",
    "\n",
    "\n",
    "#testing backpropagation\n",
    "grad = dMSEdx(np.matrix([6.0]).T, np.matrix([6.055]).T)\n",
    "grad\n",
    "perceptron.layer_backprop(grad)\n",
    "perceptron.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.90406599 2.71322907 1.28069543]]\n"
     ]
    }
   ],
   "source": [
    "p = Layer(activation = 'sigmoid',h_size=3,output_size=1,lr=0.5) # p for perceptron\n",
    "for epoch in range(300):\n",
    "    for i in range(100):\n",
    "        target_vec = np.matrix(target[i]).T\n",
    "        x_vec = np.matrix(x_input[i]).T\n",
    "        y_vec = p.layer_forward(x_vec)\n",
    "        grad = dMSEdx(y_vec,target_vec)\n",
    "        p.layer_backprop(grad)\n",
    "print(p.weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "layer_backprop() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7dbe013b1ae8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_perceptron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdMSEdx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmy_perceptron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_backprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: layer_backprop() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# Method to train a layer. We can later add more layers to form a neural network \n",
    "my_perceptron = Layer(activation='sigmoid',h_size=3,output_size=1)\n",
    "print(np.matrix(target[0]).T)\n",
    "print(np.matrix(x_input[0]).T)\n",
    "for i in range(len(x_input)):\n",
    "    target_vec = np.matrix(target[i]).T\n",
    "    x_vec = np.matrix(x_input[i]).T\n",
    "    y_vec = my_perceptron.layer_forward(x_vec)\n",
    "    grad = dMSEdx(y_vec,target_vec)\n",
    "    my_perceptron.layer_backprop(grad,0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
